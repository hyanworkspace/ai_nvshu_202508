from transformers import BertModel, BertTokenizer
import torch
import pickle
import os

# è¯»å–chinese_list.txt
with open('knowledge_base/chinese_list.txt', 'r', encoding='utf-8') as f:
    chinese_list = list(f.read()) + ['ï¼Œ', 'ã€‚', 'ï¼', 'ï¼Ÿ']


with open('knowledge_base/word_vectors.pkl', 'rb') as f:
    word_vectors_dict = pickle.load(f)

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ - ä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹
def load_bert_model():
    """åŠ è½½BERTæ¨¡å‹ï¼Œä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹"""
    try:
        # é¦–å…ˆå°è¯•åŠ è½½æœ¬åœ°æ¨¡å‹
        local_model_path = "models/bert-base-chinese"
        if os.path.exists(local_model_path):
            print("ğŸ“‚ åŠ è½½æœ¬åœ°BERTæ¨¡å‹...")
            model = BertModel.from_pretrained(local_model_path)
            tokenizer = BertTokenizer.from_pretrained(local_model_path)
        else:
            print("ğŸŒ æœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨ï¼Œä»ç½‘ç»œä¸‹è½½...")
            model = BertModel.from_pretrained('bert-base-chinese')
            tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„GPU
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model = model.to(device)
        
        print(f"âœ… BERTæ¨¡å‹åŠ è½½æˆåŠŸï¼Œä½¿ç”¨è®¾å¤‡: {device}")
        return model, tokenizer, device
        
    except Exception as e:
        print(f"âŒ BERTæ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        return None, None, None

# åŠ è½½æ¨¡å‹
bert_chinese, tokenizer_cn, device = load_bert_model()


class WordVectorManager:
    _instance = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(WordVectorManager, cls).__new__(cls)
            cls._instance.vectors = {}

            # åˆå§‹åŒ–å‘é‡
            for word in chinese_list:
                # inputs = tokenizer(word, return_tensors='pt').to(device)
                # outputs = bert_chinese(**inputs)
                cls._instance.vectors[word] = word_vectors_dict[word]
        return cls._instance
    
    def get_vector(self, word):
        if word not in self.vectors:
            inputs = tokenizer_cn(word, return_tensors='pt').to(device)
            outputs = bert_chinese(**inputs)
            self.vectors[word] = outputs.last_hidden_state[0].mean(0).detach().cpu().numpy()
        return self.vectors[word]
    
    def __getitem__(self, word):
        return self.get_vector(word)
    
    def items(self):
        return self.vectors.items()
    
    def keys(self):
        return self.vectors.keys()
    
    def values(self):
        return self.vectors.values()

# # åˆ›å»ºå…¨å±€å®ä¾‹
word_vectors = WordVectorManager()